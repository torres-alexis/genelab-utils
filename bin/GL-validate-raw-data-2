#!/usr/bin/env python

"""
GL-validate-raw-data-2: Refactored GeneLab Raw Data Validation Script

Expected Input:
- Raw FASTQ files (gzipped) in the current working directory
- Optionally, an md5 file for input files

Expected Output:
- Renamed FASTQ files (if needed)
- <GLDS_ID>-raw-validation-summary.tsv: Summary table of validation results
- <GLDS_ID>-raw-validation-tool-versions.txt: Tool versions and protocol text
- raw_md5sum_<assay>.txt: md5sums for all files
- MultiQC output files (HTML, data dir, zipped)
- SLURM batch script and log files (if --slurm is used)

Implementation Notes:
- The script is monolithic and stateful, with logic for different data types (paired-end, single-end, ATACseq, single-cell)
- Uses subprocess to call external tools (gzip, fastq_info, fastqc, multiqc, md5sum, zip)
- Designed for use in a Linux environment with all dependencies available in PATH
- For SLURM, generates a batch script and submits it with sbatch
- All output files are written to the current working directory

Usage Example:
    GL-validate-raw-data -g GLDS-480 -n 3 --assay bulkRNAseq
    GL-validate-raw-data -g GLDS-PE-test -n 3 --assay AmpSeq --slurm
"""

import argparse
import os
import glob
import re
import subprocess
import pandas as pd
import hashlib
import shutil
import zipfile

def parse_args():
    parser = argparse.ArgumentParser(
        description="Refactored GeneLab raw data validation and QC pipeline."
    )
    parser.add_argument("-g", "--GLDS-ID", required=True, help="GLDS ID (e.g. 'GLDS-480')")
    parser.add_argument("-n", "--number-of-samples", required=True, type=int, help="Expected number of samples")
    parser.add_argument("-a", "--assay", required=True, help="Assay type")
    parser.add_argument("-m", "--md5-file", default=None, help="Optional md5 file")
    parser.add_argument("-s", "--single-ended", action="store_true", help="Flag for single-end sequencing")
    parser.add_argument("--ATACseq", action="store_true", help="Flag for ATACseq data")
    parser.add_argument("--single-cell", action="store_true", help="Flag for single-cell data")
    parser.add_argument("--number-of-read-files-per-sample", type=int, choices=[3,4], help="Number of read files per sample (for single-cell)")
    parser.add_argument("-k", "--keep-fastqc-files", action="store_true", help="Keep FastQC files")
    parser.add_argument("--slurm", action="store_true", help="Submit job through SLURM")
    parser.add_argument("--nodename", default=None, help="SLURM node name")
    parser.add_argument("--mem", default=None, help="Slurm GB memory to use")
    parser.add_argument("--exclude_nodename", default=None, help="SLURM node exclusion")
    parser.add_argument("--queue", default="normal", help="SLURM queue")
    parser.add_argument("--HRremoved-suffix", action="store_true", help="Expect 'HRremoved' in file suffixes")
    return parser.parse_args()

# Assay suffixes for file naming
assay_suffixes = {
    "AmpSeq": "GLAmpSeq",
    "bulkRNAseq": "GLbulkRNAseq",
    "metabolomics": "GLmetabolomics",
    "metagenomics": "GLmetagenomics",
    "metatranscriptomics": "GLmetatranscriptomics",
    "MethylSeq": "GLMethylSeq",
    "microarray": "GLmicroarray",
    "nanoporeRNAseq": "GLnanoporeRNAseq",
    "proteomics": "GLproteomics",
    "scATACseq": "GLscATACseq",
    "scRNAseq": "GLscRNAseq",
    "smallRNAseq": "GLsmallRNAseq",
    "snATACseq": "GLsnATACseq",
    "snRNAseq": "GLsnRNAseq",
    "ST": "GLSpatialTranscriptomics",
    "targetNanoporeDNASeq": "GLTargetNanoporeDNAseq",
    "targetSeq": "GLtargetSeq",
    "targetRNAseq": "GLtargetRNAseq",
    "WGS": "GLwgs"
}

# --- File designation patterns (can be adjusted as needed) ---
R1_designations = ["_R1_", "_R1.", "-R1.", "-R1-", ".R1.", "_1."]
R2_designations = ["_R2_", "_R2.", "-R2.", "-R2-", ".R2.", "_2."]
R3_designations = ["_R3_", "_R3.", "-R3.", "-R3-", ".R3.", "_3."]
R4_designations = ["_R4_", "_R4.", "-R4.", "-R4-", ".R4.", "_4."]
extensions = [".fq", ".fastq"]

# --- Functions to get standard suffixes based on args ---
def get_standard_GL_R1_suffix(hrremoved):
    return "_R1_HRremoved_raw.fastq.gz" if hrremoved else "_R1_raw.fastq.gz"
def get_standard_GL_R2_suffix(hrremoved):
    return "_R2_HRremoved_raw.fastq.gz" if hrremoved else "_R2_raw.fastq.gz"
def get_standard_GL_SE_suffix(hrremoved):
    return "_HRremoved_raw.fastq.gz" if hrremoved else "_raw.fastq.gz"
def get_standard_GL_R3_suffix(hrremoved):
    # If you want to support HRremoved for R3, uncomment below
    # return "_R3_HRremoved_raw.fastq.gz" if hrremoved else "_R3_raw.fastq.gz"
    return "_R3_raw.fastq.gz"
def get_standard_GL_R4_suffix():
    return "_R4_raw.fastq.gz"

# --- Output file name templates (functions, not variables, for flexibility) ---
def get_md5_output_file(assay):
    return f"raw_md5sum_{assay_suffixes[assay]}.txt"
def get_md5_check_output_file(glds_id):
    return f"{glds_id}-raw-md5-check-results.txt"
def get_fastqc_threads():
    return 8  # 8 threads = 2GB RAM for FastQC

def get_multiqc_output_prefix(assay):
    return f"raw_multiqc_{assay_suffixes[assay]}"
def get_multiqc_data_dir(assay):
    return get_multiqc_output_prefix(assay) + "_data"
def get_multiqc_html(assay):
    # NOTE: just .html, not _report.html
    return get_multiqc_output_prefix(assay) + ".html"

def get_versions_used_file(glds_id):
    return f"{glds_id}-raw-validation-tool-versions.txt"
def get_log_out(glds_id):
    return f"{glds_id}-raw-fastqc-multiqc-log.txt"
def get_output_table_filename(glds_id):
    return f"{glds_id}-raw-validation-summary.tsv"

gzip_or_fastq_format_check_failed_message = "Failed gzip test and/or fastq-format check."

# SLURM output file templates (functions)
def get_sbatch_file(glds_id):
    return f"{glds_id}-raw-validation.slurm"
def get_slurm_out_file(glds_id):
    return f"{glds_id}-raw-validation-slurm.out"
def get_slurm_job_name(glds_id):
    return f"{glds_id}-raw-validation"

def get_slurm_mem(mem):
    return str(mem) if mem else "20000"

def get_slurm_queue(queue):
    return str(queue) if queue else "normal"

def discover_and_map_files(num_samples, single_ended, atacseq, single_cell, num_read_files_per_sample, hrremoved_suffix):
    """
    Discover gzipped FASTQ files in the current directory, map them to sample IDs,
    and check that the number of files matches expectations.
    Returns: dict {sample_id: [file1, file2, ...]}
    Raises ValueError on mismatch or ambiguity.
    """
    # Find all gzipped files
    gz_files = glob.glob("*.gz")
    # Only keep those with .fq or .fastq in the name
    gz_files = [f for f in gz_files if any(ext in f for ext in extensions)]
    num_files = len(gz_files)

    # Helper to extract sample ID
    def extract_sample_id(filename, designation):
        # Remove the designation and everything after it
        pattern = re.escape(designation) + ".*$"
        return re.sub(pattern, "", filename)

    # Mapping logic
    sample_map = {}

    if single_ended:
        if num_files != num_samples:
            raise ValueError(f"Expected {num_samples} single-end read files, found {num_files}.")
        # Figure out which extension is used
        used_ext = None
        for ext in extensions:
            if ext in gz_files[0]:
                used_ext = ext
                break
        if not used_ext:
            raise ValueError(f"No recognized FASTQ extension in files: {gz_files}")
        for f in gz_files:
            sample_id = re.sub(re.escape(used_ext) + ".*$", "", f)
            sample_id = re.sub("_raw$", "", sample_id)
            sample_map[sample_id] = [f]
        return sample_map

    # ATACseq or single-cell with 3 files/sample
    if atacseq or (single_cell and num_read_files_per_sample == 3):
        if num_files != num_samples * 3:
            raise ValueError(f"Expected {num_samples * 3} files, found {num_files}.")
        R1_files = [f for f in gz_files if any(d in f for d in R1_designations)]
        R2_files = [f for f in gz_files if any(d in f for d in R2_designations)]
        R3_files = [f for f in gz_files if any(d in f for d in R3_designations)]
        if not (len(R1_files) == len(R2_files) == len(R3_files) == num_samples):
            raise ValueError("Mismatch in R1/R2/R3 file counts.")
        # Find which designation is used
        R1_design = next(d for d in R1_designations if d in R1_files[0])
        R2_design = next(d for d in R2_designations if d in R2_files[0])
        R3_design = next(d for d in R3_designations if d in R3_files[0])
        for f1, f2, f3 in zip(R1_files, R2_files, R3_files):
            sid = extract_sample_id(f1, R1_design)
            sample_map[sid] = [f1, f2, f3]
        return sample_map

    # single-cell with 4 files/sample
    if single_cell and num_read_files_per_sample == 4:
        if num_files != num_samples * 4:
            raise ValueError(f"Expected {num_samples * 4} files, found {num_files}.")
        R1_files = [f for f in gz_files if any(d in f for d in R1_designations)]
        R2_files = [f for f in gz_files if any(d in f for d in R2_designations)]
        R3_files = [f for f in gz_files if any(d in f for d in R3_designations)]
        R4_files = [f for f in gz_files if any(d in f for d in R4_designations)]
        if not (len(R1_files) == len(R2_files) == len(R3_files) == len(R4_files) == num_samples):
            raise ValueError("Mismatch in R1/R2/R3/R4 file counts.")
        R1_design = next(d for d in R1_designations if d in R1_files[0])
        R2_design = next(d for d in R2_designations if d in R2_files[0])
        R3_design = next(d for d in R3_designations if d in R3_files[0])
        R4_design = next(d for d in R4_designations if d in R4_files[0])
        for f1, f2, f3, f4 in zip(R1_files, R2_files, R3_files, R4_files):
            sid = extract_sample_id(f1, R1_design)
            sample_map[sid] = [f1, f2, f3, f4]
        return sample_map

    # Typical paired-end
    if num_files != num_samples * 2:
        raise ValueError(f"Expected {num_samples * 2} paired-end files, found {num_files}.")
    R1_files = [f for f in gz_files if any(d in f for d in R1_designations)]
    R2_files = [f for f in gz_files if any(d in f for d in R2_designations)]
    if not (len(R1_files) == len(R2_files) == num_samples):
        raise ValueError("Mismatch in R1/R2 file counts.")
    R1_design = next(d for d in R1_designations if d in R1_files[0])
    R2_design = next(d for d in R2_designations if d in R2_files[0])
    for f1, f2 in zip(R1_files, R2_files):
        sid = extract_sample_id(f1, R1_design)
        sample_map[sid] = [f1, f2]
    return sample_map

def rename_files_to_convention(sample_map, hrremoved_suffix):
    """
    Rename files to GeneLab convention if needed.
    Returns a new mapping with updated filenames.
    """
    updated_map = {}
    for sample_id, files in sample_map.items():
        new_files = []
        for i, f in enumerate(files):
            # Determine expected suffix
            if len(files) == 1:
                expected_suffix = get_standard_GL_SE_suffix(hrremoved_suffix)
            elif len(files) == 2:
                expected_suffix = get_standard_GL_R1_suffix(hrremoved_suffix) if i == 0 else get_standard_GL_R2_suffix(hrremoved_suffix)
            elif len(files) == 3:
                if i == 0:
                    expected_suffix = get_standard_GL_R1_suffix(hrremoved_suffix)
                elif i == 1:
                    expected_suffix = get_standard_GL_R2_suffix(hrremoved_suffix)
                else:
                    expected_suffix = get_standard_GL_R3_suffix(hrremoved_suffix)
            elif len(files) == 4:
                if i == 0:
                    expected_suffix = get_standard_GL_R1_suffix(hrremoved_suffix)
                elif i == 1:
                    expected_suffix = get_standard_GL_R2_suffix(hrremoved_suffix)
                elif i == 2:
                    expected_suffix = get_standard_GL_R3_suffix(hrremoved_suffix)
                else:
                    expected_suffix = get_standard_GL_R4_suffix()
            else:
                raise ValueError(f"Unexpected number of files for sample {sample_id}: {len(files)}")
            expected_name = f"{sample_id}{expected_suffix}"
            if f != expected_name:
                print(f"Renaming {f} -> {expected_name}")
                os.rename(f, expected_name)
                new_files.append(expected_name)
            else:
                new_files.append(f)
        updated_map[sample_id] = new_files
    return updated_map

def get_map_dict():
    gz_files = glob.glob('*.gz')
    gz_files = [f for f in gz_files if any(ext in f for ext in extensions)]
    forward = [f for f in gz_files if any(d in f for d in R1_designations)]
    reverse = [f for f in gz_files if any(d in f for d in R2_designations)]
    map_dict = {}
    for f in forward:
        key = f.split(R1_designations[0])[0]
        map_dict[key] = [f]
    for f in reverse:
        key = f.split(R2_designations[0])[0]
        if key in map_dict:
            map_dict[key].append(f)
    return map_dict

def start_validation_output_table(map_dict, renamed_map_dict=None, data_type_cols=None):
    files_count_dict = {k: len(v) for k, v in map_dict.items()}
    out_tab = pd.DataFrame.from_dict(files_count_dict, orient='index', columns=['num_read_files'])
    if data_type_cols is None:
        data_type_cols = [f'orig_read_filename']
    new_tab = pd.DataFrame.from_dict(map_dict, orient='index', columns=data_type_cols)
    out_tab = pd.concat([out_tab, new_tab], axis=1)
    if renamed_map_dict is not None:
        renamed_tab = pd.DataFrame.from_dict(renamed_map_dict, orient='index', columns=[c.replace('orig', 'new') for c in data_type_cols])
        out_tab = pd.concat([out_tab, renamed_tab], axis=1)
    return out_tab

def check_gzip_integrity(map_dict, out_tab, gzip_cols):
    gzip_test_dict = {}
    for key in map_dict:
        results = []
        for f in map_dict[key]:
            res = subprocess.run(['gzip', '-t', f], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            results.append("PASS" if res.returncode == 0 else "FAIL")
        gzip_test_dict[key] = results
    new_tab = pd.DataFrame.from_dict(gzip_test_dict, orient='index', columns=gzip_cols)
    out_tab = pd.concat([out_tab, new_tab], axis=1)
    return out_tab

def compute_md5(filename):
    hash_md5 = hashlib.md5()
    with open(filename, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def add_md5_columns(map_dict, out_tab, md5_cols):
    md5_dict = {}
    for key in map_dict:
        md5s = []
        for f in map_dict[key]:
            md5s.append(compute_md5(f))
        md5_dict[key] = md5s
    new_tab = pd.DataFrame.from_dict(md5_dict, orient='index', columns=md5_cols)
    out_tab = pd.concat([out_tab, new_tab], axis=1)
    return out_tab

def write_md5sum_file_from_summary(summary_path, assay):
    df = pd.read_csv(summary_path, sep='\t')
    md5_lines = []
    md5_cols = [c for c in df.columns if c.endswith('_md5') or c == 'read_file_md5']
    new_name_cols = [c for c in df.columns if c.startswith('new_') or c == 'new_filename']
    orig_name_cols = [c for c in df.columns if c.startswith('orig_') or c == 'orig_read_filename']
    for _, row in df.iterrows():
        for i, md5_col in enumerate(md5_cols):
            if i < len(new_name_cols):
                fname = row[new_name_cols[i]]
                if fname == 'not-renamed' and i < len(orig_name_cols):
                    fname = row[orig_name_cols[i]]
            else:
                fname = row[orig_name_cols[i]]
            md5 = row[md5_col]
            md5_lines.append(f"{md5}  {fname}")
    outname = f"raw_md5sum_{assay}.txt"
    with open(outname, 'w') as f:
        f.write('\n'.join(md5_lines) + '\n')

def parse_md5_file(md5_file):
    md5_map = {}
    with open(md5_file) as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            parts = line.split()
            if len(parts) >= 2:
                md5, fname = parts[0], parts[1]
                md5_map[os.path.basename(fname)] = md5
    return md5_map

def add_md5_check_columns(out_tab, md5_cols, new_name_cols, orig_name_cols, md5_map):
    for i, md5_col in enumerate(md5_cols):
        check_col = md5_col.replace('_md5', '_md5_check')
        results = []
        for idx, row in out_tab.iterrows():
            if i < len(new_name_cols):
                fname = row[new_name_cols[i]]
                if fname == 'not-renamed' and i < len(orig_name_cols):
                    fname = row[orig_name_cols[i]]
            else:
                fname = row[orig_name_cols[i]]
            md5 = row[md5_col]
            expected = md5_map.get(os.path.basename(fname))
            if expected is None:
                results.append('NO_MATCH')
            elif md5 == expected:
                results.append('PASS')
            else:
                results.append('FAIL')
        out_tab[check_col] = results
    return out_tab

def write_versions_used_file(glds_id):
    def get_version(cmd, parse=None):
        try:
            out = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            if parse:
                return parse(out)
            # Default: use stdout, fallback to stderr
            return out.stdout.strip() or out.stderr.strip()
        except Exception:
            return 'not found'

    # fastq_utils version
    def parse_fastq_utils(out):
        for line in (out.stderr or '').splitlines():
            if line.startswith('fastq_utils'):
                return line.strip()
        return 'fastq_utils not found'
    fastq_utils_version = get_version(['fastq_info', '-h'], parse_fastq_utils)
    # fastqc version
    fastqc_version = get_version(['fastqc', '--version'])
    # multiqc version
    def parse_multiqc(out):
        # multiqc --version returns 'multiqc, version X.Y.Z'
        return out.stdout.strip().replace(", version ", " v")
    multiqc_version = get_version(['multiqc', '--version'], parse_multiqc)

    versions_used_file = f"{glds_id}-raw-validation-tool-versions.txt"
    with open(versions_used_file, "w") as out_file:
        out_file.write(str(fastq_utils_version) + "\n" + str(fastqc_version) + "\n" + str(multiqc_version) + "\n")
        out_file.write("\nProtocol text:\n\n")
        out_file.write(
            "Fastq format was checked with fastq_utils v" + str(fastq_utils_version).replace("fastq_utils ", "").replace("\n", "") + ". " +
            "Quality assessment of reads was performed with FastQC " + str(fastqc_version).replace("FastQC ", "").replace("\n", "") +
            " and reports were combined with MultiQC " + str(multiqc_version).replace("multiqc ", "").replace("\n", "") + ".\n\n"
        )

def run_fastqc_on_files(file_list, outdir, threads=1):
    os.makedirs(outdir, exist_ok=True)
    for f in file_list:
        subprocess.run(['fastqc', '-o', outdir, '-t', str(threads), f], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

def run_multiqc(fastqc_dir, outdir, args):
    outdir = os.getcwd()
    subprocess.run(['multiqc', fastqc_dir, '-o', outdir], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    # Find the generated HTML and data dir
    htmls = glob.glob('multiqc*.html')
    datas = glob.glob('*_data')
    html_target = get_multiqc_html(args.assay)
    data_target = get_multiqc_data_dir(args.assay)
    if htmls:
        os.rename(htmls[0], html_target)
    if datas:
        os.rename(datas[0], data_target)
        # Zip the data dir and remove the original
        shutil.make_archive(data_target, 'zip', data_target)
        shutil.rmtree(data_target)

def parse_multiqc_and_add_to_summary(summary_path, out_tab, updated_map, data_type_cols):
    import glob, os
    import pandas as pd
    # Find multiqc data dir or zip
    multiqc_data_dirs = glob.glob('*_data')
    if not multiqc_data_dirs:
        zips = glob.glob('*_data.zip')
        if zips:
            import zipfile
            with zipfile.ZipFile(zips[0], 'r') as zip_ref:
                zip_ref.extractall('tmp_multiqc_data')
            multiqc_data_dir = 'tmp_multiqc_data'
        else:
            print('[WARN] No MultiQC data dir found.')
            return out_tab
    else:
        multiqc_data_dir = multiqc_data_dirs[0]
    # Parse fastqc stats
    fastqc_stats_path = os.path.join(multiqc_data_dir, 'multiqc_fastqc.txt')
    if not os.path.exists(fastqc_stats_path):
        print('[WARN] MultiQC fastqc stats file not found.')
        if os.path.exists('tmp_multiqc_data'):
            import shutil
            shutil.rmtree('tmp_multiqc_data')
        return out_tab
    fastqc_stats = pd.read_csv(fastqc_stats_path, sep='\t', usecols=[0,4,7,10])
    fastqc_stats.columns = ['sample', 'total_sequences', 'length_range', 'avg_length']
    fastqc_stats.set_index('sample', inplace=True)
    # Count FastQC reports per sample (by unique_ID)
    fastqc_report_count = {}
    for key in updated_map:
        count = 0
        for f in updated_map[key]:
            sample_id = os.path.basename(f).replace('.fastq.gz', '').replace('.fq.gz', '')
            if sample_id in fastqc_stats.index:
                count += 1
        fastqc_report_count[key] = count
    out_tab['num_fastqc_reports_in_multiqc_report'] = [fastqc_report_count[k] for k in out_tab['unique_ID']]
    # Build per-sample dicts
    counts_dict, length_range_dict, avg_length_dict = {}, {}, {}
    num_reads_equal = {}
    read_lengths_equal = {}
    for key in updated_map:
        files = updated_map[key]
        sample_ids = [os.path.basename(f).replace('.fastq.gz', '').replace('.fq.gz', '') for f in files]
        counts = []
        length_ranges = []
        avg_lengths = []
        for sid in sample_ids:
            try:
                counts.append(round(float(fastqc_stats.loc[sid, 'total_sequences'])))
                length_ranges.append(fastqc_stats.loc[sid, 'length_range'])
                avg_lengths.append(fastqc_stats.loc[sid, 'avg_length'])
            except Exception:
                counts.append('NA')
                length_ranges.append('NA')
                avg_lengths.append('NA')
        counts_dict[key] = counts
        length_range_dict[key] = length_ranges
        avg_length_dict[key] = avg_lengths
        # Add equal columns for paired-end and 3/4-read
        if len(counts) >= 2:
            num_reads_equal[key] = 'YES' if counts[0] == counts[1] else 'NO'
            read_lengths_equal[key] = 'YES' if length_ranges[0] == length_ranges[1] and avg_lengths[0] == avg_lengths[1] else 'NO'
        else:
            num_reads_equal[key] = ''
            read_lengths_equal[key] = ''
    # Add columns to out_tab
    for i, col in enumerate(data_type_cols):
        col_base = col.replace('orig_', '').replace('_filename', '')
        if len(data_type_cols) == 1:
            out_tab['read_counts'] = [counts_dict[k][0] for k in out_tab['unique_ID']]
            out_tab['read_length_range'] = [length_range_dict[k][0] for k in out_tab['unique_ID']]
            out_tab['avg_read_length'] = [avg_length_dict[k][0] for k in out_tab['unique_ID']]
        else:
            out_tab[f'{col_base}_num_reads'] = [counts_dict[k][i] for k in out_tab['unique_ID']]
            out_tab[f'{col_base}_read_length_range'] = [length_range_dict[k][i] for k in out_tab['unique_ID']]
            out_tab[f'{col_base}_avg_read_length'] = [avg_length_dict[k][i] for k in out_tab['unique_ID']]
    # Add the equal columns for paired-end
    if len(data_type_cols) == 2:
        out_tab['R1_and_R2_num_reads_equal'] = [num_reads_equal[k] for k in out_tab['unique_ID']]
        out_tab['R1_and_R2_read_lengths_equal'] = [read_lengths_equal[k] for k in out_tab['unique_ID']]
    # Clean up temp dir if used
    if os.path.exists('tmp_multiqc_data'):
        import shutil
        shutil.rmtree('tmp_multiqc_data')
    return out_tab

def run_fastq_check(file1, file2=None):
    if file2 is None:
        check_out = subprocess.run(['fastq_info', file1], capture_output=True, text=True)
    else:
        check_out = subprocess.run(['fastq_info', file1, file2], capture_output=True, text=True)
    if check_out.returncode == 0:
        return 'PASS'
    else:
        # Try to extract error message
        for line in check_out.stderr.splitlines():
            if line.startswith('ERROR'):
                return f'FAIL - {line}'
        return 'FAIL'

def add_fastq_format_check_columns(updated_map, out_tab, data_type_cols):
    import pandas as pd
    fastq_check_dicts = []
    paired_check = False
    if len(data_type_cols) == 1:
        # Single-end
        colnames = ['fastq_format_check']
        for key in updated_map:
            file1 = updated_map[key][0]
            result = run_fastq_check(file1)
            fastq_check_dicts.append([result])
    else:
        # Paired-end or more
        colnames = [f'{c.replace("orig_", "").replace("_filename", "")}_fastq_format_check' for c in data_type_cols]
        if len(data_type_cols) == 2:
            colnames.append('paired_fastq_format_check')
            paired_check = True
        for key in updated_map:
            row = []
            files = updated_map[key]
            for f in files:
                row.append(run_fastq_check(f))
            if paired_check:
                row.append(run_fastq_check(files[0], files[1]))
            fastq_check_dicts.append(row)
    fastq_check_df = pd.DataFrame(fastq_check_dicts, columns=colnames, index=out_tab.index)
    out_tab = pd.concat([out_tab, fastq_check_df], axis=1)
    return out_tab

def main():
    args = parse_args()
    try:
        print("[1/8] Discovering and mapping files...")
        sample_map = discover_and_map_files(
            num_samples=args.number_of_samples,
            single_ended=args.single_ended,
            atacseq=args.ATACseq,
            single_cell=args.single_cell,
            num_read_files_per_sample=args.number_of_read_files_per_sample,
            hrremoved_suffix=args.HRremoved_suffix
        )
        print("[2/8] Renaming files to convention (if needed)...")
        renamed_map_dict = {}
        updated_map = {}
        data_type_cols = []
        gzip_cols = []
        md5_cols = []
        nfiles = len(next(iter(sample_map.values())))
        if args.single_ended:
            data_type_cols = ['orig_read_filename']
            gzip_cols = ['read_file_gzip_test']
            md5_cols = ['read_file_md5']
        elif args.ATACseq or (args.single_cell and args.number_of_read_files_per_sample == 3):
            data_type_cols = ['orig_R1_filename', 'orig_R2_filename', 'orig_R3_filename']
            gzip_cols = ['R1_gzip_test', 'R2_gzip_test', 'R3_gzip_test']
            md5_cols = ['R1_md5', 'R2_md5', 'R3_md5']
        elif args.single_cell and args.number_of_read_files_per_sample == 4:
            data_type_cols = ['orig_R1_filename', 'orig_R2_filename', 'orig_R3_filename', 'orig_R4_filename']
            gzip_cols = ['R1_gzip_test', 'R2_gzip_test', 'R3_gzip_test', 'R4_gzip_test']
            md5_cols = ['R1_md5', 'R2_md5', 'R3_md5', 'R4_md5']
        else:
            data_type_cols = ['orig_R1_filename', 'orig_R2_filename']
            gzip_cols = ['R1_gzip_test', 'R2_gzip_test']
            md5_cols = ['R1_md5', 'R2_md5']
        for sample_id, files in sample_map.items():
            new_files = []
            renamed_cols = []
            for i, f in enumerate(files):
                if args.single_ended:
                    expected_suffix = get_standard_GL_SE_suffix(args.HRremoved_suffix)
                elif args.ATACseq or (args.single_cell and nfiles == 3):
                    expected_suffix = [get_standard_GL_R1_suffix(args.HRremoved_suffix), get_standard_GL_R2_suffix(args.HRremoved_suffix), get_standard_GL_R3_suffix(args.HRremoved_suffix)][i]
                elif args.single_cell and nfiles == 4:
                    expected_suffix = [get_standard_GL_R1_suffix(args.HRremoved_suffix), get_standard_GL_R2_suffix(args.HRremoved_suffix), get_standard_GL_R3_suffix(args.HRremoved_suffix), get_standard_GL_R4_suffix()][i]
                else:
                    expected_suffix = get_standard_GL_R1_suffix(args.HRremoved_suffix) if i == 0 else get_standard_GL_R2_suffix(args.HRremoved_suffix)
                expected_name = f"{sample_id}{expected_suffix}"
                if f != expected_name:
                    os.rename(f, expected_name)
                    new_files.append(expected_name)
                    renamed_cols.append(expected_name)
                else:
                    new_files.append(f)
                    renamed_cols.append("not-renamed")
            updated_map[sample_id] = new_files
            renamed_map_dict[sample_id] = renamed_cols
        print("[3/8] Running gzip integrity checks...")
        out_tab = start_validation_output_table(updated_map, renamed_map_dict, data_type_cols)
        out_tab = check_gzip_integrity(updated_map, out_tab, gzip_cols)
        print("[4/8] Running fastq format checks...")
        out_tab = add_fastq_format_check_columns(updated_map, out_tab, data_type_cols)
        print("[5/8] Calculating md5sums...")
        out_tab = add_md5_columns(updated_map, out_tab, md5_cols)
        out_tab.reset_index(inplace=True)
        out_tab = out_tab.rename(columns={'index': 'unique_ID'})
        summary_path = get_output_table_filename(args.GLDS_ID)
        out_tab.to_csv(summary_path, sep='\t', index=False)
        print("[6/8] Running FastQC on all files...")
        all_files = [f for files in updated_map.values() for f in files]
        run_fastqc_on_files(all_files, 'FastQC_Outputs', threads=1)
        print("[7/8] Running MultiQC...")
        run_multiqc('FastQC_Outputs', '.', args)
        # Remove FastQC_Outputs if not keeping FastQC files
        if not args.keep_fastqc_files:
            shutil.rmtree('FastQC_Outputs', ignore_errors=True)
        print("[8/8] Parsing MultiQC output and updating summary table...")
        out_tab = parse_multiqc_and_add_to_summary(summary_path, out_tab, updated_map, data_type_cols)
        out_tab.to_csv(summary_path, sep='\t', index=False)
        print("[done] Writing md5sum file...")
        write_md5sum_file_from_summary(summary_path, args.assay)
        if args.md5_file:
            print("[done] Checking md5s against provided file...")
            md5_map = parse_md5_file(args.md5_file)
            new_name_cols = [c for c in out_tab.columns if c.startswith('new_') or c == 'new_filename']
            orig_name_cols = [c for c in out_tab.columns if c.startswith('orig_') or c == 'orig_read_filename']
            out_tab = add_md5_check_columns(out_tab, md5_cols, new_name_cols, orig_name_cols, md5_map)
            out_tab.to_csv(summary_path, sep='\t', index=False)
        print("[done] Writing tool versions and protocol text...")
        write_versions_used_file(args.GLDS_ID)
        # Reorder columns to match original as closely as possible
        orig_order = []
        # Build expected order for paired-end, single-end, etc.
        if args.single_ended:
            orig_order = [
                'unique_ID','num_read_files','orig_read_filename','new_read_filename','read_file_gzip_test','fastq_format_check','read_file_md5','read_counts','read_length_range','avg_read_length'
            ]
            if 'read_file_md5_check' in out_tab.columns:
                orig_order.append('read_file_md5_check')
        elif len(data_type_cols) == 2:
            orig_order = [
                'unique_ID','num_read_files','orig_R1_filename','orig_R2_filename','new_R1_filename','new_R2_filename','R1_gzip_test','R2_gzip_test',
                'R1_fastq_format_check','R2_fastq_format_check','paired_fastq_format_check','R1_md5','R2_md5','R1_num_reads','R1_read_length_range','R1_avg_read_length','R2_num_reads','R2_read_length_range','R2_avg_read_length',
                'R1_and_R2_num_reads_equal','R1_and_R2_read_lengths_equal','num_fastqc_reports_in_multiqc_report'
            ]
            if 'R1_md5_check' in out_tab.columns:
                orig_order += ['R1_md5_check','R2_md5_check']
        elif len(data_type_cols) == 3:
            orig_order = [
                'unique_ID','num_read_files','orig_R1_filename','orig_R2_filename','orig_R3_filename','new_R1_filename','new_R2_filename','new_R3_filename',
                'R1_gzip_test','R2_gzip_test','R3_gzip_test','R1_fastq_format_check','R2_fastq_format_check','R3_fastq_format_check','R1_md5','R2_md5','R3_md5',
                'R1_num_reads','R1_read_length_range','R1_avg_read_length','R2_num_reads','R2_read_length_range','R2_avg_read_length','R3_num_reads','R3_read_length_range','R3_avg_read_length'
            ]
            if 'R1_md5_check' in out_tab.columns:
                orig_order += ['R1_md5_check','R2_md5_check','R3_md5_check']
        elif len(data_type_cols) == 4:
            orig_order = [
                'unique_ID','num_read_files','orig_R1_filename','orig_R2_filename','orig_R3_filename','orig_R4_filename','new_R1_filename','new_R2_filename','new_R3_filename','new_R4_filename',
                'R1_gzip_test','R2_gzip_test','R3_gzip_test','R4_gzip_test','R1_fastq_format_check','R2_fastq_format_check','R3_fastq_format_check','R4_fastq_format_check','R1_md5','R2_md5','R3_md5','R4_md5',
                'R1_num_reads','R1_read_length_range','R1_avg_read_length','R2_num_reads','R2_read_length_range','R2_avg_read_length','R3_num_reads','R3_read_length_range','R3_avg_read_length','R4_num_reads','R4_read_length_range','R4_avg_read_length'
            ]
            if 'R1_md5_check' in out_tab.columns:
                orig_order += ['R1_md5_check','R2_md5_check','R3_md5_check','R4_md5_check']
        # Only reorder if all columns present
        missing = [c for c in orig_order if c not in out_tab.columns]
        if not missing:
            out_tab = out_tab[orig_order]
            out_tab.to_csv(summary_path, sep='\t', index=False)
    except Exception as e:
        print(f"[ERROR] {e}")

if __name__ == "__main__":
    main()
